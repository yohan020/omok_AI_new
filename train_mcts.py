# íŒŒì¼ëª…: train_mcts.py

import torch
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import numpy as np
from tqdm import tqdm
import os
import random

from model import ResNetActorCritic
from environment import OmokEnv
from mcts import run_mcts

# --- 1. í›ˆë ¨ ì„¤ì • (8x8 ë²„ì „) ---
BOARD_SIZE = 8              # (!!!) 8x8 ë³´ë“œ
BATCH_SIZE = 128
REPLAY_BUFFER_SIZE = 30000
EPISODES_PER_CYCLE = 20     # í•œ ì‚¬ì´í´ë‹¹ 20íŒ ëŒ€êµ­
TRAIN_EPOCHS_PER_CYCLE = 10 # í•œ ì‚¬ì´í´ë‹¹ 10íšŒ í•™ìŠµ
MCTS_SIMULATIONS = 400      # 8x8ì—ì„œëŠ” ì¶©ë¶„í•œ ê¹Šì´
C_PUCT = 1.0
MODEL_SAVE_DIR = 'models_8x8_reward' # ì €ì¥ ê²½ë¡œ

# --- 2. ë¦¬ì…‹ ë° ì´ì–´í•˜ê¸° ì„¤ì • ---
RESUME_FROM_CYCLE = 0       # (!!!) 0ë¶€í„° ìƒˆë¡œ ì‹œì‘ ê¶Œì¥
FINAL_CYCLE_GOAL = 1000     # ëª©í‘œ ì‚¬ì´í´

INITIAL_LEARNING_RATE = 0.001 # ì´ˆê¸° í•™ìŠµë¥ 
NEW_SCHEDULER_STEP = 100      # 100 ì‚¬ì´í´ë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ
# -----------------------------

def get_symmetries(state, pi):
    """ ë°ì´í„° ì¦ê°•: íšŒì „ ë° ëŒ€ì¹­ìœ¼ë¡œ 1íŒì„ 8íŒì²˜ëŸ¼ ë§Œë“¦ """
    aug_data = []
    pi_board = np.reshape(pi, (BOARD_SIZE, BOARD_SIZE))
    for i in range(4):
        state_rot = np.rot90(state, k=i, axes=(1, 2))
        pi_rot = np.rot90(pi_board, k=i)
        aug_data.append((np.ascontiguousarray(state_rot), np.ascontiguousarray(pi_rot.flatten())))
        state_flip = np.flip(state_rot, axis=2)
        pi_flip = np.fliplr(pi_rot)
        aug_data.append((np.ascontiguousarray(state_flip), np.ascontiguousarray(pi_flip.flatten())))
    return aug_data

def self_play(model, device):
    """ ìê°€ ëŒ€êµ­: ë°ì´í„° ìˆ˜ì§‘ (ê°€ìƒ ìš¸íƒ€ë¦¬ + ì¤‘ê°„ ë³´ìƒ ì ìš©) """
    replay_data = []
    env = OmokEnv(board_size=BOARD_SIZE)
    state = env.reset()
    game_history = []
    
    move_count = 0
    
    # (!!!) ê°€ìƒ ìš¸íƒ€ë¦¬ ì„¤ì •: ì´ˆë°˜ 6ìˆ˜ëŠ” ì¤‘ì•™ 4x4 (ì¸ë±ìŠ¤ 2~5) ê°•ì œ
    RESTRICT_MOVES_UNTIL = 6
    MIN_IDX, MAX_IDX = 2, 6

    while True:
        # MCTS ì‹¤í–‰ (ë…¸ì´ì¦ˆ ì¼œê¸°: íƒí—˜ ìœ ë„)
        best_action, pi_target = run_mcts(env, model, device,
                                          num_simulations=MCTS_SIMULATIONS,
                                          c_puct=C_PUCT,
                                          add_noise=True)

        if best_action == -1: break
        
        # (!!!) ê°€ìƒ ìš¸íƒ€ë¦¬ ê°•ì œ ë¡œì§
        if move_count < RESTRICT_MOVES_UNTIL:
            row, col = divmod(best_action, BOARD_SIZE)
            # AIê°€ ìš¸íƒ€ë¦¬ ë°–(êµ¬ì„)ìœ¼ë¡œ ë‚˜ê°€ë ¤ í•˜ë©´?
            if not (MIN_IDX <= row < MAX_IDX and MIN_IDX <= col < MAX_IDX):
                # ê°•ì œë¡œ ì¤‘ì•™ ë¹ˆì¹¸ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ ì„ íƒ (êµì •)
                center_candidates = []
                for r in range(MIN_IDX, MAX_IDX):
                    for c in range(MIN_IDX, MAX_IDX):
                        if env.board[r, c] == 0:
                            center_candidates.append(r * BOARD_SIZE + c)
                
                if center_candidates:
                    best_action = random.choice(center_candidates)
                    # ì •ì±… íƒ€ê²Ÿë„ ì´ ìˆ˜ê°€ 100% ì •ë‹µì¸ ê²ƒì²˜ëŸ¼ ìˆ˜ì •
                    pi_target = np.zeros(BOARD_SIZE * BOARD_SIZE)
                    pi_target[best_action] = 1.0
        
        # (!!!) ì¤‘ê°„ ë³´ìƒ íšë“
        # step í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” rewardì—ëŠ” ìŠ¹íŒ¨(+1/-1) ë¿ë§Œ ì•„ë‹ˆë¼
        # ê³µê²© ì„±ê³µ ë³´ë„ˆìŠ¤(+0.2, +0.5)ë„ í¬í•¨ë¨ (environment.py ìˆ˜ì • í•„ìˆ˜)
        next_state, immediate_reward, done = env.step(best_action)
        
        # ì—­ì‚¬ ì €ì¥: (ìƒíƒœ, MCTSí™•ë¥ , "ì´ë²ˆ ìˆ˜ì˜ ë³´ìƒ")
        game_history.append([env.get_state(), pi_target, immediate_reward])
        
        state = next_state
        move_count += 1
        
        if done:
            # ì—­ì „íŒŒ: ê²Œì„ ëì—ì„œë¶€í„° ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê°€ì¹˜(Value) ê³„ì‚°
            running_value = 0.0
            
            for i in range(len(game_history) - 1, -1, -1):
                state_hist, pi_hist, reward_hist = game_history[i]
                
                if i == len(game_history) - 1:
                    # ë§ˆì§€ë§‰ ìˆ˜ëŠ” ìŠ¹íŒ¨ ë³´ìƒ (+1.0 or -1.0 or 0)
                    running_value = reward_hist
                    if running_value > 1.0: running_value = 1.0 # ìº¡
                else:
                    # ì¤‘ê°„ ìˆ˜ëŠ” "ë¯¸ë˜ ê°€ì¹˜(ìƒëŒ€ë°© ì…ì¥ì˜ -Value)" + "ì¦‰ê° ë³´ìƒ(ê³µê²© ì ìˆ˜)"
                    # V(s) = Reward + V(s') * (-1)
                    running_value = reward_hist - running_value
                
                # ë°ì´í„° ì¦ê°• í›„ ì €ì¥
                symmetries = get_symmetries(state_hist, pi_hist)
                for sym_state, sym_pi in symmetries:
                    # running_valueê°€ ì‹ ê²½ë§ì´ ì˜ˆì¸¡í•´ì•¼ í•  ëª©í‘œê°’(z)ì´ ë¨
                    replay_data.append((sym_state, sym_pi, running_value))
            
            break
            
    return replay_data

def train_network(model, optimizer, replay_buffer, device):
    """ ì‹ ê²½ë§ í•™ìŠµ """
    sample_size = min(len(replay_buffer), BATCH_SIZE)
    if sample_size == 0: return 0.0, 0.0
        
    samples = random.sample(replay_buffer, sample_size)
    states, pis, zs = zip(*samples)
    
    state_batch = torch.tensor(np.array(states), dtype=torch.float32).to(device)
    pi_target_batch = torch.tensor(np.array(pis), dtype=torch.float32).to(device)
    z_target_batch = torch.tensor(np.array(zs), dtype=torch.float32).unsqueeze(1).to(device)

    policy_logits, value_pred = model(state_batch)
    
    # Value Loss (MSE): ì˜ˆì¸¡ ê°€ì¹˜ì™€ ì‹¤ì œ ê°€ì¹˜(ë³´ìƒ í•©ê³„) ì°¨ì´
    value_loss = F.mse_loss(value_pred, z_target_batch)
    
    # Policy Loss (Cross Entropy): ì˜ˆì¸¡ í™•ë¥ ê³¼ MCTS í™•ë¥  ì°¨ì´
    policy_loss = -torch.sum(pi_target_batch * F.log_softmax(policy_logits, dim=-1), dim=-1).mean()
    
    total_loss = value_loss + policy_loss
    
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    
    return value_loss.item(), policy_loss.item()

def main():
    if torch.cuda.is_available(): device = torch.device("cuda")
    elif torch.backends.mps.is_available(): device = torch.device("mps")
    else: device = torch.device("cpu")
    print(f"Using device: {device}")
    
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
        print(f"'{MODEL_SAVE_DIR}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    model = ResNetActorCritic(board_size=BOARD_SIZE).to(device)
    
    start_cycle = 0
    current_lr = INITIAL_LEARNING_RATE

    # ì´ì–´í•˜ê¸° ë¡œì§
    if RESUME_FROM_CYCLE > 0:
        MODEL_PATH = os.path.join(MODEL_SAVE_DIR, f'resnet_omok_model_cycle_{RESUME_FROM_CYCLE}.pth')
        try:
            model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
            print(f"ğŸ”„ ê¸°ì¡´ í›ˆë ¨ ëª¨ë¸ ë°œê²¬! ì´ì–´í•˜ê¸°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤: {MODEL_PATH}")
            start_cycle = RESUME_FROM_CYCLE
        except FileNotFoundError:
            print(f"ëª¨ë¸({MODEL_PATH})ì´ ì—†ì–´ 0ë¶€í„° ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸš€ 8x8 ë³´ë“œì—ì„œ í›ˆë ¨ì„ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤!")

    optimizer = optim.Adam(model.parameters(), lr=current_lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=NEW_SCHEDULER_STEP, gamma=0.1)
    
    print(f"   -> Cycle {start_cycle}ë¶€í„° {FINAL_CYCLE_GOAL}ê¹Œì§€ í›ˆë ¨í•©ë‹ˆë‹¤.")
    
    replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)

    for cycle in tqdm(range(start_cycle, FINAL_CYCLE_GOAL), desc="Training Progress"):
        
        # ìë™ í•™ìŠµë¥  ë¦¬ì…‹ (0ì´ ë˜ë©´ ì´ˆê¸°í™”)
        current_lr = scheduler.get_last_lr()[0]
        if current_lr < 1e-8:
            print("\n" + "="*50)
            print(f"Cycle {cycle+1}: í•™ìŠµë¥  ë¦¬ì…‹")
            optimizer = optim.Adam(model.parameters(), lr=INITIAL_LEARNING_RATE, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=NEW_SCHEDULER_STEP, gamma=0.1)
            current_lr = scheduler.get_last_lr()[0]
            print(f"   -> ìƒˆ í•™ìŠµë¥ : {current_lr:.8f}")
            print("="*50 + "\n")

        print(f"\n--- Cycle {cycle + 1}/{FINAL_CYCLE_GOAL} ---")
        
        # 1. ìê°€ ëŒ€êµ­ (ë°ì´í„° ìˆ˜ì§‘)
        model.eval()
        pbar_self_play = tqdm(range(EPISODES_PER_CYCLE), desc="Self-Playing")
        for _ in pbar_self_play:
            new_data = self_play(model, device)
            replay_buffer.extend(new_data)
        
        # 2. ì‹ ê²½ë§ í•™ìŠµ
        model.train()
        if len(replay_buffer) < BATCH_SIZE:
            print("ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            scheduler.step()
            continue
            
        pbar_train = tqdm(range(TRAIN_EPOCHS_PER_CYCLE), desc="Training Network")
        total_v_loss = 0
        total_p_loss = 0
        for _ in pbar_train:
            v_loss, p_loss = train_network(model, optimizer, replay_buffer, device)
            total_v_loss += v_loss
            total_p_loss += p_loss
            
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
            
        print(f"Avg Value Loss: {total_v_loss/TRAIN_EPOCHS_PER_CYCLE:.4f}, "
              f"Avg Policy Loss: {total_p_loss/TRAIN_EPOCHS_PER_CYCLE:.4f}, "
              f"LR: {current_lr:.8f}")

        # ëª¨ë¸ ì €ì¥ (10 ì‚¬ì´í´ë§ˆë‹¤)
        if (cycle + 1) % 10 == 0:
            save_path = os.path.join(MODEL_SAVE_DIR, f'resnet_omok_model_cycle_{cycle+1}.pth')
            torch.save(model.state_dict(), save_path)
            tqdm.write(f"\nModel saved to {save_path}")

    # ìµœì¢… ì €ì¥
    final_save_path = os.path.join(MODEL_SAVE_DIR, f'resnet_omok_model_{FINAL_CYCLE_GOAL}.pth')
    torch.save(model.state_dict(), final_save_path)
    print(f"Final model saved to {final_save_path}")

if __name__ == '__main__':
    main()
